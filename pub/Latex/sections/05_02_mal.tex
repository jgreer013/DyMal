While this paper's work is focused on interpreting general software and not specifically malware, the application to the malware and cybersecurity domain is important enough to warrant mentioning the other approaches towards malware analysis and classification, particularly as many focus their attention in the area of malware vs non-malware rather than general program analysis.

Identifying key behavioral characteristics of software is one of the main goals of this work, but specifically to do so in such a way that is automatically applicable to new programs while still maintaining human understanding. There are many methods of representing the structure or behavior of programs in a formal way, such as Abstract Syntax Trees, Control Flow Graphs, Call Graphs, and Dependency Graphs. Many of these methods have been used to great effect in understanding information flow through a program; however, many of these methods are either intensive, do not work on binaries/assemblies, or are difficult to use in creating direct comparisons between two programs.

One major work which seeks to categorize specifically malware behavior is Malware Attribute Enumeration and Characterization (MAEC) \cite{maec}. Essentially an encyclopedia of higher-level malware attributes, this work characterizes major malware families based on a set of common attributes held by malware; however, this work is generally human curated, which can potentially lead to error, but more importantly will not work with large amounts of new programs, as new malware can be created and distributed more quickly than a human expert can sufficiently study it. If a system were able to automatically extract these high-level features, then this structure would be very useful in discussing and classifying malware.

There are many works which seek to identify and classify malware based on their key features. Rieck \textit{et al.} \cite{rieck2008learning} extracted generalized behavior features (such as opening an IRC connection) of malware while it runs in a sandboxed environment. They then clustered the malware based on these components using an SVM with a bag-of-words model. Their approach allows for an interpretable understanding of their features and enables a feature ranking on the extracted dynamic components, something not often seen in the other approaches. SVMs could reasonably be extended to multi-class classification, though the features and their rankings would become more unwieldy as the number of classes grows. Since we hope to approach a general software domain where there could be any number of classes or behaviors, this seemed infeasible for our purposes.

Lindorfer \textit{et al.} \cite{lindorfer2011detecting} developed a system to detect environment-sensitive malware, as some malware has developed the ability to recognize when it is being run in a sandboxed environment and thus behave differently, avoiding detection or study. They do this by analyzing programs run in multiple sandboxes multiple times and detecting differences between them using an Information Theory-based approach with Jaccard distances. While this gives an idea as to a program's environmental behavior, it fails to indicate anything intrinsic about its behavior or purpose.

Santos \textit{et al.} \cite{santos2013opem} utilized a combined static-dynamic approach to detecting malware, combining opcode frequency and an execution trace as data. They apply a variety of models to this data and comparing their performances. As the main purpose of the work was to show that a combined static-dynamic approach performs better than either approach alone, it provides no greater understanding into differentiating malware behavior. Sun \textit{et al.} \cite{sun_signature_2006} introduced a patent for a system which automatically generates a malware signature based on what malware it is classified as, but detail is not given in terms of what differentiates the malware or malware classes. Bailey \textit{et al.} \cite{bailey2007automated} sought to identify malware by using a file compression of dynamically generated system state logs and cluster the files based on this metric; however, again, analyzing the entire log would prove infeasible for a human expert, and the compression metric leaves limited interpretability in terms of the similarity between two files beyond the score itself.

Most major works seem primarily concerned with model performance, with relatively little work done in trying to create or assist in creating new insights into understanding program behavior. Enabling these insights requires maintaining interpretability throughout the process, thereby limiting model selection to those which act with interpretable features.