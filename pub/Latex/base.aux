\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand\HyperFirstAtBeginDocument{\AtBeginDocument}
\HyperFirstAtBeginDocument{\ifx\hyper@anchor\@undefined
\global\let\oldcontentsline\contentsline
\gdef\contentsline#1#2#3#4{\oldcontentsline{#1}{#2}{#3}}
\global\let\oldnewlabel\newlabel
\gdef\newlabel#1#2{\newlabelxx{#1}#2}
\gdef\newlabelxx#1#2#3#4#5#6{\oldnewlabel{#1}{{#2}{#3}}}
\AtEndDocument{\ifx\hyper@anchor\@undefined
\let\contentsline\oldcontentsline
\let\newlabel\oldnewlabel
\fi}
\fi}
\global\let\hyper@last\relax 
\gdef\HyperFirstAtBeginDocument#1{#1}
\providecommand\HyField@AuxAddToFields[1]{}
\providecommand\HyField@AuxAddToCoFields[2]{}
\citation{agrawal2016trends}
\citation{statista}
\citation{maier2014divide}
\citation{bosu2017collusive}
\citation{palmer_2019}
\@writefile{toc}{\contentsline {section}{\numberline {1}Introduction}{1}{section.1}}
\newlabel{introduction}{{1}{1}{Introduction}{section.1}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2}Background}{1}{section.2}}
\newlabel{background}{{2}{1}{Background}{section.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.1}Third-Party Software}{1}{subsection.2.1}}
\citation{maier2014divide}
\citation{arntz_ccleaner_2017}
\citation{maier2014divide}
\citation{statista}
\citation{kaggle}
\citation{ghosh_behavior_1999}
\citation{Sherwood_large_2002}
\citation{Bowring_active_2004}
\citation{Mohaisen_systems_2014}
\citation{jacob2008behavioral}
\citation{shabtai2012andromaly}
\citation{wu2012droidmat}
\citation{burguera2011crowdroid}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2}Malware}{2}{subsection.2.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3}Importance of Interpretability}{2}{subsection.2.3}}
\@writefile{toc}{\contentsline {section}{\numberline {3}Previous Work}{2}{section.3}}
\newlabel{previous_work}{{3}{2}{Previous Work}{section.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.1}Understanding Program Behavior}{2}{subsection.3.1}}
\citation{maec}
\citation{rieck2008learning}
\citation{lindorfer2011detecting}
\citation{santos2013opem}
\citation{sun_signature_2006}
\citation{bailey2007automated}
\citation{doshi2017towards}
\citation{vellido2012making}
\citation{hainmueller2014kernel}
\citation{chen2016infogan}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.2}Malware Analysis and Classification}{3}{subsection.3.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {3.3}Interpretable Machine Learning}{3}{subsection.3.3}}
\citation{objdump}
\citation{geeks}
\citation{bao2014byteweight}
\@writefile{toc}{\contentsline {section}{\numberline {4}Procedure}{4}{section.4}}
\newlabel{procedure}{{4}{4}{Procedure}{section.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {1}{\ignorespaces Flowchart detailing the flow and transformation of information across the entire process. Documents are transformed and analyzed using LDA, and then the transformations are undone to allow for fine-grain interpretability.}}{4}{figure.1}}
\newlabel{fig:flow}{{1}{4}{Flowchart detailing the flow and transformation of information across the entire process. Documents are transformed and analyzed using LDA, and then the transformations are undone to allow for fine-grain interpretability}{figure.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.1}Data and Assembly Instructions}{4}{subsection.4.1}}
\citation{mikolov2013distributed}
\@writefile{lof}{\contentsline {figure}{\numberline {2}{\ignorespaces Annotated version of Linear Search's search function in assembly. Similarities were found in sorting functions with the primary difference being sections of data modification and multiple for-loops.}}{5}{figure.2}}
\newlabel{fig:search}{{2}{5}{Annotated version of Linear Search's search function in assembly. Similarities were found in sorting functions with the primary difference being sections of data modification and multiple for-loops}{figure.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.2}Word2Vec Embeddings and Clustering}{5}{subsection.4.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {3}{\ignorespaces T-SNE plot of small-scale data Word2Vec results.}}{5}{figure.3}}
\newlabel{fig:tsne}{{3}{5}{T-SNE plot of small-scale data Word2Vec results}{figure.3}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.3}N-Grams}{5}{subsection.4.3}}
\citation{blei2003latent}
\citation{griffiths2004hierarchical}
\citation{yuan2015lightlda}
\@writefile{lof}{\contentsline {figure}{\numberline {4}{\ignorespaces Hierarchical Clustering results of the small dataset.}}{6}{figure.4}}
\newlabel{fig:clust}{{4}{6}{Hierarchical Clustering results of the small dataset}{figure.4}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {4.4}LDA and hLDA}{6}{subsection.4.4}}
\@writefile{toc}{\contentsline {section}{\numberline {5}Results}{6}{section.5}}
\newlabel{results}{{5}{6}{Results}{section.5}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {5}{\ignorespaces Side-by-side comparison of Linear Search and Selection Sort. Selection sort can be thought to contain the entirety of linear search (as a slightly modified linear search is used in the selection sort algorithm), so these two programs will be the most similar while still having different purposes.}}{6}{figure.5}}
\newlabel{fig:small_lda}{{5}{6}{Side-by-side comparison of Linear Search and Selection Sort. Selection sort can be thought to contain the entirety of linear search (as a slightly modified linear search is used in the selection sort algorithm), so these two programs will be the most similar while still having different purposes}{figure.5}{}}
\@writefile{lot}{\contentsline {table}{\numberline {1}{\ignorespaces Final results of small-scale LDA model. The top topics are determined based on the learned document-topic distribution. Focus on specific topics is contained in Figure \ref  {fig:small_lda}. Hyperparameters for these results were the following: Number of topics = 13, $\alpha = 1e-10, \beta = 0.1$}}{6}{table.1}}
\newlabel{tab:small_lda_tab}{{1}{6}{Final results of small-scale LDA model. The top topics are determined based on the learned document-topic distribution. Focus on specific topics is contained in Figure \ref {fig:small_lda}. Hyperparameters for these results were the following: Number of topics = 13, $\alpha = 1e-10, \beta = 0.1$}{table.1}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.1}Small-Scale Results}{6}{subsection.5.1}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.1}LDA Results}{6}{subsubsection.5.1.1}}
\@writefile{lot}{\contentsline {table}{\numberline {2}{\ignorespaces Final results of small-scale hLDA model. Each document is a leaf node in a hierarchy of topics, so those at the same leaf node are most similar. Hyperparameters for these results were the following: Number of levels = 3, number of samples = 500, $\alpha = 10, \gamma = 1, \beta = 0.1$}}{7}{table.2}}
\newlabel{tab:small_hlda_tab}{{2}{7}{Final results of small-scale hLDA model. Each document is a leaf node in a hierarchy of topics, so those at the same leaf node are most similar. Hyperparameters for these results were the following: Number of levels = 3, number of samples = 500, $\alpha = 10, \gamma = 1, \beta = 0.1$}{table.2}{}}
\@writefile{toc}{\contentsline {subsubsection}{\numberline {5.1.2}hLDA Results}{7}{subsubsection.5.1.2}}
\@writefile{toc}{\contentsline {subsection}{\numberline {5.2}Large-Scale Results}{7}{subsection.5.2}}
\@writefile{lof}{\contentsline {figure}{\numberline {6}{\ignorespaces Documents placed into bins corresponding to their highest-weighted topic. All sorting and searching programs are placed into the same bin, with no other programs being placed within it.}}{7}{figure.6}}
\newlabel{fig:large_lda}{{6}{7}{Documents placed into bins corresponding to their highest-weighted topic. All sorting and searching programs are placed into the same bin, with no other programs being placed within it}{figure.6}{}}
\citation{hype}
\citation{chang2009reading}
\citation{teh2005sharing}
\citation{guidedlda}
\@writefile{lot}{\contentsline {table}{\numberline {3}{\ignorespaces Final results of large-scale LDA model. The top topics are determined based on the learned document-topic distribution. Hyperparameters for these results were the following: Number of topics = 100, $\alpha = 0.001, \beta = 0.001$. The - symbol means that no additional topics were formed as part of its weighting (due to their weights being too small).}}{8}{table.3}}
\newlabel{tab:large_lda_tab}{{3}{8}{Final results of large-scale LDA model. The top topics are determined based on the learned document-topic distribution. Hyperparameters for these results were the following: Number of topics = 100, $\alpha = 0.001, \beta = 0.001$. The - symbol means that no additional topics were formed as part of its weighting (due to their weights being too small)}{table.3}{}}
\@writefile{toc}{\contentsline {section}{\numberline {6}Discussion}{8}{section.6}}
\newlabel{discussion}{{6}{8}{Discussion}{section.6}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.1}LDA and hLDA}{8}{subsection.6.1}}
\@writefile{toc}{\contentsline {subsection}{\numberline {6.2}Large-Scale LDA}{8}{subsection.6.2}}
\citation{santos2013opem}
\citation{kaggle}
\bibstyle{./bib/IEEEtran.bst}
\bibdata{./bib/biblio.bib}
\bibcite{agrawal2016trends}{1}
\bibcite{statista}{2}
\bibcite{maier2014divide}{3}
\bibcite{bosu2017collusive}{4}
\@writefile{toc}{\contentsline {section}{\numberline {7}Future Work}{9}{section.7}}
\newlabel{future_work}{{7}{9}{Future Work}{section.7}{}}
\@writefile{toc}{\contentsline {section}{\numberline {8}Conclusion}{9}{section.8}}
\newlabel{conclusion}{{8}{9}{Conclusion}{section.8}{}}
\@writefile{toc}{\contentsline {section}{References}{9}{section*.2}}
\bibcite{palmer_2019}{5}
\bibcite{arntz_ccleaner_2017}{6}
\bibcite{kaggle}{7}
\bibcite{ghosh_behavior_1999}{8}
\bibcite{Sherwood_large_2002}{9}
\bibcite{Bowring_active_2004}{10}
\bibcite{Mohaisen_systems_2014}{11}
\bibcite{jacob2008behavioral}{12}
\bibcite{shabtai2012andromaly}{13}
\bibcite{wu2012droidmat}{14}
\bibcite{burguera2011crowdroid}{15}
\bibcite{maec}{16}
\bibcite{rieck2008learning}{17}
\bibcite{lindorfer2011detecting}{18}
\bibcite{santos2013opem}{19}
\bibcite{sun_signature_2006}{20}
\bibcite{bailey2007automated}{21}
\bibcite{doshi2017towards}{22}
\bibcite{vellido2012making}{23}
\bibcite{hainmueller2014kernel}{24}
\bibcite{chen2016infogan}{25}
\bibcite{objdump}{26}
\bibcite{geeks}{27}
\bibcite{bao2014byteweight}{28}
\bibcite{mikolov2013distributed}{29}
\bibcite{blei2003latent}{30}
\bibcite{griffiths2004hierarchical}{31}
\bibcite{yuan2015lightlda}{32}
\bibcite{hype}{33}
\bibcite{chang2009reading}{34}
\bibcite{teh2005sharing}{35}
\bibcite{guidedlda}{36}
\@writefile{toc}{\contentsline {section}{Biographies}{11}{IEEEbiography.0}}
\@writefile{toc}{\contentsline {subsection}{Jeremiah Greer}{11}{IEEEbiography.1}}
\@writefile{toc}{\contentsline {subsection}{Rashmi Jha}{11}{IEEEbiography.2}}
\@writefile{toc}{\contentsline {subsection}{Anca Ralescu}{11}{IEEEbiography.3}}
\@writefile{toc}{\contentsline {subsection}{Temesguen Messay-Kebede}{11}{IEEEbiography.4}}
\@writefile{toc}{\contentsline {subsection}{David Kapp}{11}{IEEEbiography.5}}
